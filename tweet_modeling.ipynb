{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "tweet_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hadizh/tweet_topic_modeling/blob/master/tweet_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oJItgScOBdY",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyp8wPHkrcP9",
        "colab_type": "text"
      },
      "source": [
        "## Setting up Colab environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16kGi0-YstaA",
        "colab_type": "text"
      },
      "source": [
        "## Scraping Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoexyiQcOVu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install retry\n",
        "!pip install GetOldTweets3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zA5VRqvOBdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import datetime\n",
        "\n",
        "import GetOldTweets3 as got\n",
        "from retry import retry"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mycc9WihOBdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define function to scrape tweets\n",
        "@retry(SystemExit, delay=60, backoff=2, max_delay=8*60)\n",
        "def scrape_tweets(company, *, start_date, end_date):\n",
        "  tweetCriteria = got.manager.TweetCriteria().setUsername(company) \\\n",
        "                                               .setSince(start_date) \\\n",
        "                                               .setUntil(end_date) \\\n",
        "                                               .setLang(\"en\")\n",
        "  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "  return tweets\n",
        "\n",
        "# Define function to save company tweets to separate csv files\n",
        "def save_tweets_to_csv(company, *, start_date, end_date):\n",
        "    output_file = PROJECT_PATH + '/tweets/{}.csv'.format(company)\n",
        "    try:\n",
        "        with open(output_file, \"x\", encoding=\"utf8\") as out:\n",
        "            tweets = scrape_tweets(company, start_date=start_date, end_date=end_date)\n",
        "            csv_file = csv.writer(out, lineterminator='\\n')\n",
        "            csv_file.writerow(['date', 'username', 'to', 'replies', 'retweets',\n",
        "                               'favorites', 'text', 'geo', 'mentions', 'hashtags',\n",
        "                               'id', 'permalink'])\n",
        "            for t in tweets:\n",
        "                data = [t.date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                        t.username,\n",
        "                        t.to or '',\n",
        "                        t.replies,\n",
        "                        t.retweets,\n",
        "                        t.favorites,\n",
        "                        t.text,\n",
        "                        t.geo,\n",
        "                        t.mentions,\n",
        "                        t.hashtags,\n",
        "                        t.id,\n",
        "                        t.permalink]\n",
        "                csv_file.writerow(map(str, data))\n",
        "            out.flush()\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "def delete_tweets_to_people(company):\n",
        "  file = PROJECT_PATH + '/tweets/{}.csv'.format(company)\n",
        "  with open(file, \"r\") as input_file:\n",
        "    reader = csv.DictReader(input_file, lineterminator='\\n')\n",
        "    output_rows = [row for row in reader if not row['to']]\n",
        "  with open(file, \"w\") as output_file:\n",
        "    header = ['date', 'username', 'to', 'replies', 'retweets', 'favorites', \n",
        "              'text', 'geo', 'mentions', 'hashtags', 'id', 'permalink']\n",
        "    writer = csv.DictWriter(output_file, fieldnames=header)\n",
        "    writer.writeheader()\n",
        "    for row in output_rows:\n",
        "      writer.writerow(row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st8qjWoIOBdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Open file containing all companies we are interested in\n",
        "with open(PROJECT_PATH + '/companies.txt') as cf:\n",
        "    companies = sorted(line.rstrip() for line in cf)\n",
        "companies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiQj5PR-OBdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save tweets for each company from the past three years\n",
        "start_date = (datetime.datetime.now() \n",
        "              - datetime.timedelta(days=365*3)).strftime(\"%Y-%m-%d\")\n",
        "end_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "for company in companies:\n",
        "  save_tweets_to_csv(company, start_date=start_date, end_date=end_date)\n",
        "  delete_tweets_to_people(company)\n",
        "\n",
        "# Show tweets per company\n",
        "for company in companies:\n",
        "  file = PROJECT_PATH + '/tweets/{}.csv'.format(company)\n",
        "  with open(file, \"r\") as f:\n",
        "    print(\"{} : {}\".format(company, len(f.readlines()) - 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgjmLds6OBdm",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning Data\n",
        "\n",
        "Before we begin performing the topic modeling, we must clean and transform our data. The process of cleaning data makes use of the open source code from this guide: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_5yMV17OUzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade gensim\n",
        "!pip install --upgrade pyLDAvis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4hb8ZmeOBdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sqlite3\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# spacy for lemmatization\n",
        "import spacy\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLO7MZ9eOBdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import a corpus of stopwords to use later in lemmatization\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OORn6Lr_OBds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load tweet data for all companies\n",
        "def load_tweets(company):\n",
        "  df = pd.read_csv(PROJECT_PATH + '/tweets/{}.csv'.format(company))\n",
        "  return df.text.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2tNcVnUOBdx",
        "colab_type": "text"
      },
      "source": [
        "We first must clean our data by formatting, removing stop words, and lemmatizing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdm3yk4COBdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define functions for formatting, stopwords, bigrams, and lemmatization\n",
        "def format_texts(texts):\n",
        "    new_lines = re.compile(r'\\s+')\n",
        "    single_quotes = re.compile(r\"\\'\")\n",
        "    hyperlinks = re.compile(r'http\\S+')\n",
        "    mentions = re.compile(r'\\S+@\\S+')\n",
        "    hashtags = re.compile(r'#')\n",
        "    ampersands = re.compile(r'&amp;')\n",
        "    camel_case_split = re.compile(r'((?<=[a-z])[A-Z]|(?<=\\S)[A-Z](?=[a-z]))')\n",
        "    \n",
        "    # Strip newlines, possesives, and hashtags\n",
        "    texts = [re.sub(new_lines, ' ', str(text)) for text in texts]\n",
        "    texts = [re.sub(single_quotes, \"\", str(text)) for text in texts]\n",
        "    texts = [re.sub(hyperlinks, '', str(text)) for text in texts]\n",
        "    texts = [re.sub(mentions, '', str(text)) for text in texts]\n",
        "    texts = [re.sub(hashtags, '', str(text)) for text in texts]\n",
        "    texts = [re.sub(ampersands, '', str(text)) for text in texts]\n",
        "    texts = [re.sub(camel_case_split, r' \\1', str(text)) for text in texts]\n",
        "    return texts\n",
        "\n",
        "def remove_stopwords(texts):\n",
        "    return [[word for word \n",
        "             in simple_preprocess(str(doc), deacc=True, max_len=30) \n",
        "             if word not in stop_words] \n",
        "            for doc in texts]\n",
        "\n",
        "def make_ngrams(texts):\n",
        "    bigram = gensim.models.Phrases(texts, \n",
        "                                   min_count=4, \n",
        "                                   threshold=0.5, \n",
        "                                   scoring=gensim.models.phrases.npmi_scorer)\n",
        "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "    return [bigram_mod[doc] for doc in texts]\n",
        "\n",
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEbzfFbrOBd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define function for cleaning tweets\n",
        "def clean_tweets(company):\n",
        "    # Format text\n",
        "    data_words = format_texts(load_tweets(company))\n",
        "\n",
        "    # Remove Stop Words\n",
        "    data_words_nostops = remove_stopwords(data_words)\n",
        "\n",
        "    # Make Bigrams\n",
        "    data_words_ngrams = make_ngrams(data_words_nostops)\n",
        "\n",
        "    # Do lemmatization keeping only noun, adj, vb, adv\n",
        "    data_lemmatized = lemmatization(data_words_ngrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "    return data_lemmatized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQKjKRiIOBd2",
        "colab_type": "text"
      },
      "source": [
        "## Modeling and Visualization\n",
        "\n",
        "To perform the topic modeling, we apply LDA to extract topics. The following analysis makes use of the open source code from this guide: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/. We also visualize our findings, using the open source code from this guide: https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z1xWpu8OBd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define functions for combining texts and creating a corpus and id2word from the texts\n",
        "def combine_texts(*args):\n",
        "    sum_texts = []\n",
        "    for text in args:\n",
        "        sum_texts.extend(text)\n",
        "    return sum_texts\n",
        "\n",
        "def create_corpus_and_id2word(texts):\n",
        "    # Create Dictionary\n",
        "    id2word = corpora.Dictionary(texts)\n",
        "    id2word.filter_extremes(no_below=2, no_above=0.9)\n",
        "\n",
        "    # Term Document Frequency\n",
        "    corpus = [id2word.doc2bow(text) for text in texts]\n",
        "    return (corpus, id2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxFSbEvf4tIy",
        "colab_type": "text"
      },
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO5ywbr6OBd9",
        "colab_type": "text"
      },
      "source": [
        "We now can build an LDA model. To do this, we use the LDA model from MALLET, and iterate through different models to find a model that has a suitable coherence score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7IxUB7LRri8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
        "!unzip -o mallet-2.0.8.zip\n",
        "import os\n",
        "os.environ[\"MALLET_HOME\"] = \"/content/mallet-2.0.8\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv3hVypGmmWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare a sqlite table to store our results\n",
        "conn = sqlite3.connect(PROJECT_PATH + \"/data.db\")\n",
        "cur = conn.cursor()\n",
        "create_lda_models_table_sql = \"\"\"\n",
        "                                CREATE TABLE IF NOT EXISTS lda_models (\n",
        "                                  id INTEGER PRIMARY KEY,\n",
        "                                  company TEXT NOT NULL,\n",
        "                                  num_topics INTEGER NOT NULL,\n",
        "                                  corpus BLOB,\n",
        "                                  id2word BLOB,\n",
        "                                  optimize_interval INTEGER DEFAULT 20,\n",
        "                                  model BLOB,\n",
        "                                  coherence REAL NOT NULL,\n",
        "                                  created_at DATETIME DEFAULT CURRENT_TIMESTAMP\n",
        "                                );\n",
        "                              \"\"\"\n",
        "cur.execute(create_lda_models_table_sql)\n",
        "conn.commit()\n",
        "\n",
        "# Define a function for updating lda models in our sqlite table\n",
        "def update_lda_models(company, num_topics, corpus, id2word, optimize_interval, model, coherence):\n",
        "  update_lda_models_sql = \"\"\"\n",
        "                            INSERT INTO lda_models\n",
        "                             (company, num_topics, corpus, id2word, optimize_interval, model, coherence) \n",
        "                            VALUES (:company,:num_topics,:corpus,:id2word,:optimize_interval,:model,:coherence);\n",
        "                          \"\"\"\n",
        "  row = {\"company\": company,\n",
        "         \"num_topics\": num_topics,\n",
        "         \"corpus\": corpus,\n",
        "         \"id2word\": id2word,\n",
        "         \"optimize_interval\": optimize_interval,\n",
        "         \"model\": model,\n",
        "         \"coherence\": coherence}\n",
        "  cur.execute(update_lda_models_sql, row)\n",
        "  conn.commit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMX8TXGFOBd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build LDA model for companies\n",
        "def compute_coherence_values(*companies, start=2, stop=12, step=2, force=False):\n",
        "  # Check if we already computed or forced\n",
        "    company_name = '+'.join(sorted(list(companies)))\n",
        "    total_companies = pd.read_sql_query(\"SELECT DISTINCT company FROM lda_models;\", conn).company.tolist()\n",
        "    if company_name in total_companies and force is False:\n",
        "      return\n",
        "    msg = \"Training for {}\".format(\n",
        "        company_name,\n",
        "        start,\n",
        "        stop,\n",
        "        step,\n",
        "    )\n",
        "    print(msg)\n",
        "    # Get text, corpus, and id2word for companies\n",
        "    texts = combine_texts(*[clean_tweets(company) for company in companies])\n",
        "    corpus, id2word = create_corpus_and_id2word(texts)\n",
        "\n",
        "    # Iterate through the range of num_topics and create mallet models\n",
        "    mallet_path = 'mallet-2.0.8/bin/mallet'\n",
        "    optimize_interval = 20\n",
        "    for num_topics in range(start, stop, step):\n",
        "        mallet_model = gensim.models.wrappers.LdaMallet(mallet_path, \n",
        "                                                 corpus=corpus, \n",
        "                                                 num_topics=num_topics, \n",
        "                                                 id2word=id2word, \n",
        "                                                 optimize_interval=optimize_interval)\n",
        "        coherencemodel = CoherenceModel(model=mallet_model, \n",
        "                                        texts=texts, \n",
        "                                        dictionary=id2word, \n",
        "                                        coherence='c_v')\n",
        "        coherence = coherencemodel.get_coherence()\n",
        "        # Add to lda_models db\n",
        "        corpus_blob = sqlite3.Binary(pickle.dumps(corpus))\n",
        "        id2word_blob = sqlite3.Binary(pickle.dumps(id2word))\n",
        "        model_blob = sqlite3.Binary(pickle.dumps(mallet_model))\n",
        "        update_lda_models(company_name, num_topics, corpus_blob, id2word_blob, 20, model_blob, coherence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnolCkJeOBeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create all the needed models\n",
        "start = 2\n",
        "stop = 20\n",
        "step = 1\n",
        "with open(PROJECT_PATH + \"/models.txt\", \"r\") as f:\n",
        "  models = [line.rstrip().split(',') for line in f]\n",
        "models\n",
        "for model in models:\n",
        "  compute_coherence_values(*model, start=start, stop=stop, step=step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDCCbIPl1Uai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show all distinct companies currently in table\n",
        "pd.read_sql_query(\"SELECT DISTINCT company FROM lda_models;\", conn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3s-W76X44D_",
        "colab_type": "text"
      },
      "source": [
        "### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH0-s5bN1zC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to get latest models from sqlite table as a dataframe\n",
        "def get_latest_rows(*companies):\n",
        "  company_name = '+'.join(sorted(list(companies)))\n",
        "  query = \"\"\"SELECT a.*\n",
        "             FROM lda_models AS a\n",
        "             INNER JOIN (\n",
        "                SELECT company, num_topics, corpus, id2word, optimize_interval, coherence, MAX(created_at) AS max_created_at\n",
        "                FROM lda_models\n",
        "                WHERE company = :company\n",
        "                GROUP BY company, num_topics\n",
        "             ) AS b\n",
        "             ON a.company = b.company\n",
        "             AND a.num_topics = b.num_topics\n",
        "             AND a.created_at = b.max_created_at\n",
        "             ORDER BY a.num_topics ASC\n",
        "          \"\"\"\n",
        "  return pd.read_sql_query(query, conn, params={\"company\": company_name})\n",
        "\n",
        "# Define a function to get a specific model for a company\n",
        "def get_ldamodel_corpus_id2word(*companies, num_topics):\n",
        "  company_name = '+'.join(sorted(list(companies)))\n",
        "  query = \"\"\"SELECT a.*\n",
        "             FROM lda_models AS a\n",
        "             INNER JOIN (\n",
        "                SELECT company, num_topics, corpus, id2word, optimize_interval, coherence, MAX(created_at) AS max_created_at\n",
        "                FROM lda_models\n",
        "                WHERE company = :company AND num_topics = :num_topics\n",
        "                GROUP BY company, num_topics\n",
        "             ) AS b\n",
        "             ON a.company = b.company\n",
        "             AND a.num_topics = b.num_topics\n",
        "             AND a.created_at = b.max_created_at\n",
        "             ORDER BY a.num_topics ASC\n",
        "          \"\"\"\n",
        "  row = pd.read_sql_query(query, conn, params={\"company\": company_name,\n",
        "                                               \"num_topics\": num_topics})\n",
        "  return (gensim.models.wrappers.ldamallet.malletmodel2ldamodel(pickle.loads(row.model.to_list()[0])),\n",
        "          pickle.loads(row.corpus.to_list()[0]),\n",
        "          pickle.loads(row.id2word.tolist()[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAdzoTdVabTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_topic_contributions(ldamodel, corpus):\n",
        "  # Init output\n",
        "  sent_topics_df = pd.DataFrame()\n",
        "  # Get main topic in each document\n",
        "  for i, row_list in enumerate(ldamodel[corpus]):\n",
        "      row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "      # print(row)\n",
        "      row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "      # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "      for j, (topic_num, _) in enumerate(row):\n",
        "          if j == 0:  # => dominant topic\n",
        "              sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), \"\"]), ignore_index=True)\n",
        "          else:\n",
        "              break  \n",
        "  sent_topics_df.columns = ['Dominant_Topic', 'Topic_Keywords']\n",
        "  total_count = sent_topics_df.shape[0]\n",
        "\n",
        "  # Group the dataframe\n",
        "  grouped_df = sent_topics_df.groupby(['Dominant_Topic'], as_index=False).agg(['count'])\n",
        "  grouped_df.columns = list(map(''.join, grouped_df.columns.values))\n",
        "  grouped_df.reset_index(level=0, inplace=True)\n",
        "  grouped_df.columns = ['Dominant_Topic','Fraction_of_Documents']\n",
        "  grouped_df['Fraction_of_Documents'] = grouped_df['Fraction_of_Documents'].apply(lambda x: x / total_count)\n",
        "\n",
        "  # Add keywords to topic\n",
        "  grouped_df['Topic_Keywords'] = grouped_df['Dominant_Topic'].apply(\n",
        "      lambda num: \", \".join([word for word, _ in ldamodel.show_topic(int(num))])\n",
        "  )\n",
        "  grouped_df['Dominant_Topic'] = grouped_df['Dominant_Topic'].astype(int)\n",
        "  grouped_df.set_index('Dominant_Topic', inplace=True)\n",
        "  return grouped_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1CmDX4BVkFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "import os\n",
        "\n",
        "# Define a function to generate a word cloud\n",
        "def generate_word_cloud(ldamodel, *companies, num_topics):\n",
        "  company_name = '+'.join(sorted(list(companies)))\n",
        "  topics = ldamodel.show_topics(num_topics=num_topics, formatted=False)\n",
        "  directory = PROJECT_PATH + '/{}'.format(company_name)\n",
        "  os.makedirs(directory) if not os.path.isdir(directory) else None\n",
        "  for i in range(len(topics)):\n",
        "      plt.figure()\n",
        "      topic_words = dict(topics[i][1])\n",
        "      cloud = WordCloud(prefer_horizontal=1.0, \n",
        "                        background_color=\"white\", \n",
        "                        relative_scaling=0.5,\n",
        "                        colormap=\"Dark2\",\n",
        "                        max_words=10)\n",
        "      cloud = cloud.generate_from_frequencies(topic_words)\n",
        "      plt.gca().imshow(cloud)\n",
        "      plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "      plt.gca().axis('off')\n",
        "      plt.tight_layout()\n",
        "      plt.show()\n",
        "      # Save to file\n",
        "      save_file = directory + '/topic_{}_word_cloud.png'.format(i)\n",
        "      if not os.path.isfile(save_file):\n",
        "        cloud.to_file(save_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjbgAYSZ096l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define a function generate word counts/word weights within a topic\n",
        "def generate_word_counts(ldamodel, *companies, num_topics):\n",
        "  company_name = '+'.join(sorted(list(companies)))\n",
        "  topics = ldamodel.show_topics(num_topics=num_topics, formatted=False)\n",
        "  texts = combine_texts(*[clean_tweets(company) for company in companies])\n",
        "  flat_texts = [w for w_list in texts for w in w_list]\n",
        "  counter = Counter(flat_texts)\n",
        "\n",
        "  out = []\n",
        "  for i, topic in topics:\n",
        "      for word, weight in topic:\n",
        "          out.append([word, i , weight, counter[word]])\n",
        "  return pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])\n",
        "\n",
        "# Define a function to generate a bar graph from word counts\n",
        "def generate_word_count_bar_graph(df, *companies, num_topics):\n",
        "  company_name = '+'.join(sorted(list(companies)))\n",
        "  directory = PROJECT_PATH + '/{}/word_distributions'.format(company_name)\n",
        "  os.makedirs(directory) if not os.path.isdir(directory) else None\n",
        "  cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "  for i in range(0, num_topics): # TODO len topics\n",
        "    plt.figure()\n",
        "    ax = plt.gca()\n",
        "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i % len(cols)], width=0.5, alpha=0.3, label='Word Count')\n",
        "    ax_twin = ax.twinx()\n",
        "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i % len(cols)], width=0.2, label='Weights')\n",
        "    ax.set_ylabel('Word Count', color=cols[i % len(cols)])\n",
        "\n",
        "    # Set scaled ylims\n",
        "    i_df = df.loc[df['topic_id'] == i]\n",
        "    ax_twin.set_ylim(0, 1.2*i_df.importance.max())\n",
        "    ax.set_ylim(0, 1.2*i_df.word_count.max())\n",
        "\n",
        "    ax.set_title('Topic: ' + str(i), color=cols[i % len(cols)], fontsize=16)\n",
        "    ax.tick_params(axis='y', left=False)\n",
        "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
        "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
        "    # Save to file\n",
        "    save_file = directory + '/topic_{}_word_distribution.png'.format(i)\n",
        "    if not os.path.isfile(save_file):\n",
        "      plt.savefig(save_file, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoorevBZIDGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "\n",
        "# Define a function for running pyLDAvis and saving the internal svg\n",
        "def run_pyLDAvis(*companies, num_topics):\n",
        "  company_name = '+'.join(sorted(list(companies)))\n",
        "  directory = PROJECT_PATH + '/{}/pyLDAvis'.format(company_name)\n",
        "  os.makedirs(directory) if not os.path.isdir(directory) else None\n",
        "\n",
        "  vis = pyLDAvis.gensim.prepare(*get_ldamodel_corpus_id2word(*companies, num_topics=num_topics), sort_topics=False)\n",
        "  # Save to file\n",
        "  save_file = directory + '/{}.html'.format(company_name)\n",
        "  if not os.path.isfile(save_file):\n",
        "    pyLDAvis.save_html(vis, save_file)\n",
        "  \n",
        "  # TODO: Save svg inside to a separate pdf file\n",
        "  return vis"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}